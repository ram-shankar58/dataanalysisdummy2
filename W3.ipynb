import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the training data
train_data = pd.read_csv("/kaggle/input/inputfiles/train.csv")

# EDA: Summary statistics
print(train_data.describe())

# EDA: Check for missing values
print(train_data.isnull().sum())

# EDA: Visualize the distribution of the target variable 'House'
plt.figure(figsize=(8, 6))
sns.countplot(x='House', data=train_data)
plt.title('Distribution of House')
plt.show()

# EDA: Visualize the distribution of numerical features
numerical_cols = ['Intelligence', 'Loyalty', 'Affinity to Dark Arts', 'Height', 'Kindness']
plt.figure(figsize=(12, 8))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(2, 3, i)
    sns.histplot(data=train_data, x=col, kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

# EDA: Visualize the relationship between numerical features and the target variable
plt.figure(figsize=(12, 8))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(2, 3, i)
    sns.boxplot(x='House', y=col, data=train_data)
    plt.title(f'Relationship between {col} and House')
plt.tight_layout()
plt.show()

# EDA: Visualize the relationship between categorical features and the target variable
categorical_cols = ['Blood Status', 'Gender', 'What are you curious about?',
                    'Preferred Goblet Potion', 'If you were a principal, how would you choose your students?',
                    'Species', 'Preferred Element', 'Magic Transportation Model']

plt.figure(figsize=(16, 12))
for i, col in enumerate(categorical_cols, 1):
    plt.subplot(3, 3, i)
    sns.countplot(x=col, hue='House', data=train_data)
    plt.title(f'Relationship between {col} and House')
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# Separate features and target variable for training data
X_train = train_data.drop(columns=["House"])  # Keep 'ID' and other columns except 'House'
y_train = train_data["House"]

# Define numerical and categorical columns
numerical_cols = ['Intelligence', 'Loyalty', 'Affinity to Dark Arts', 'Height', 'Kindness']
categorical_cols = ['Blood Status', 'Gender', 'What are you curious about?', 'Preferred Goblet Potion',
                    'If you were a principal, how would you choose your students?', 'Species', 'Preferred Element',
                    'Magic Transportation Model']

import matplotlib.pyplot as plt

for col in categorical_cols:
    train_data[col].value_counts().plot(kind='bar')
    plt.title(col)
    plt.show()

import seaborn as sns

corr_matrix = train_data[numerical_cols].corr()
sns.heatmap(corr_matrix, annot=True)
plt.show()


for col in categorical_cols:
    print(train_data[col].value_counts())

import numpy as np
import scipy.stats as stats

# Correlation Analysis
corr_matrix = train_data[numerical_cols].corr()
print("Correlation Matrix for Numerical Features:")
print(corr_matrix)

# Chi-Square Test
print("\nP-values from Chi-Square Test:")
for col in categorical_cols:
    contingency_table = pd.crosstab(train_data[col], train_data["House"])
    _, p, _, _ = stats.chi2_contingency(contingency_table)
    print(f"{col}: {p}")

print("IDEAL VALUE FOR CORRELATIOM:0.05 or low")  
# Preprocessing for numerical data: fill missing values with the mean and scale
numerical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", StandardScaler())
])

# Preprocessing for categorical data: fill missing values with the most frequent value and one-hot encoding
# Preprocessing for categorical data: fill missing values with the most frequent value and one-hot encoding
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(drop="first", handle_unknown='ignore'))  # Add handle_unknown='ignore'
])


# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numerical_transformer, numerical_cols),
        ("cat", categorical_transformer, categorical_cols)
    ])

# Define the model
model = RandomForestClassifier(n_estimators=100, random_state=0)

# Bundle preprocessing and modeling code in a pipeline
clf = Pipeline(steps=[('preprocessor', preprocessor),
                      ('model', model)])

# Preprocessing of training data, fit model 
clf.fit(X_train, y_train)

# Load the test data
test_data = pd.read_csv("/kaggle/input/inputfiles/Kaggle_test.csv")

# Retain 'ID' in the test dataset
X_test = test_data.copy()  # Make a copy of the test data

# Preprocessing of test data, get predictions
y_test_pred = clf.predict(X_test)

# Add the predictions to the test data
test_data["House"] = y_test_pred

# Save only 'ID' and 'Predicted_House' to a CSV file
test_data[['ID', 'House']].to_csv("predicted_test_data.csv", index=False)

# You can now use the "predicted_test_data.csv" file to submit your predictions.
